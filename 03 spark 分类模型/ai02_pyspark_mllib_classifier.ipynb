{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        使用决策树二元分类分析StumbleUpon数据集，预测网页是暂时性（Ephemeral）或是长青的（Evergreen），\n",
    "    并且调校参数找出最佳参数组合，提高预测准确度。\n",
    "        决策树的优点：条例清晰、方法简单、易于理解、使用范围广等。\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    决策树介绍：\n",
    "        当我们使用决策树分类算法训练数据后，会以 feature(特征字段)与label(标签字段)建立决策树。如下图所示：\n",
    "    使用 湿度 与 气压（feature 特征字段）来判断天气为“晴”或“雨”（label 标签字段，也就是预测的目标）。\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./datas/decisionTree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        当我们使用历史数据执行训练时会建立决策树。可是决策树不可能无限成长，因此我们必须限制它的最大分支与深度，\n",
    "    所以必须设置下列参数：\n",
    "        -a. maxBins 参数：\n",
    "            决策时每一个节点最大分支数\n",
    "        -b. maxDepth 参数：\n",
    "            决策树最大深度\n",
    "        -c. Impurity 参数：\n",
    "            决策树分裂节点时的方法，为什么选择特征进行分支\n",
    "        当树的父节点在分裂子节点时，以什么方法作为依据？例如，湿度以60为分割点，分为大于60或小于60；或者湿度\n",
    "    以50为分割点，分为大于50或小于50.到底哪种方式比较好呢？此时有Gini 与 Entropy 两种判断方式：\n",
    "        -i. 基于系数（Gini）：\n",
    "            由意大利统计学家Corrado Gini 发明，用于计算数值散步程度（Statistical Dispersion，统计离差）的指标。\n",
    "        决策树算法对每种特征字段分割点计算估值，选择分裂后最小的基尼指数（Gini）方式。\n",
    "        -ii. 熵（Entropy）:\n",
    "            熵（Entropy）也被用于计算机系统混乱的程度。决策树算法对每种特征字段分割点计算估值，选择分裂后最小\n",
    "        的熵（Entropy）方式。\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle数据分析竞赛平台"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Kaggle 网站介绍\n",
    "        -a. 数据分析竞赛平台，企业或研究者将大数据的问题发布到网站上，包括数据、问题说明、期望的目标、奖金等，以\n",
    "            向大众征求解决方案。\n",
    "                http://www.kaggle.com/\n",
    "        -b. 网络上任何人都可以参与大数据问题的竞赛：\n",
    "                下载数据、分析数据、运行机器学习、数据挖掘等知识，建立算法模型并解决问题，最后将结果上传到网站上。\n",
    "            如果提交申请的结果符合要求，并且在参赛者种排名第一，就可以获得奖金。\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StumbleUpon Evergreen 竞赛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    1、Kaggle 网站上一个竞赛：StumbleUpon Evergreen Classification Challenge\n",
    "        https://www.kaggle.com/c/stumbleupon\n",
    "    2、StumbleUpon是个性化的搜索引擎，会按用户的兴趣和网页评分等记录推荐给你感兴趣的网页，例如新文章、季节菜单、\n",
    "    新闻、教学等。超过数千万人使用StumbleUpon查找新网页、图片、影片.....\n",
    "    3、业务说明：\n",
    "        -a. 有些网页内容是暂时性的（ephemeral），例如季节菜单、当日股市涨跌新闻等。\n",
    "            这些文章可能只是某一段时间会对读者有意义，过了这段时间对读者就没有意义了。\n",
    "        -b. 有些网页内容是长青的（evergreen），例如理财观念、育儿知识等。\n",
    "            读者会长久对这些文章感兴趣。\n",
    "        -c. 分辨网页是暂时性（ephemeral）还是长青的（evergreen），对于StumbleUpon推荐网页给用户会有很大帮助。\n",
    "            例如 A 买卖股票，Ta可能对当日股市涨跌新闻感兴趣，可是过了一周就对这则新闻就没有兴趣了；如果是理财\n",
    "            观念的文章，读者A可能会对会长久感兴趣。\n",
    "        -d. 因此公司找来大数据分析师，负责“网页分类”大数据项目。\n",
    "            网页内容我们人类看过了，就可以大致分为暂时性的或是长青的，这就是历史数据。\n",
    "        -e. 目标就是利用机器学习（Machine Learning），通过大量网页数据进行训练来建立一个模型，并使用这个模型去\n",
    "            预测网页是属于 暂时性的 还是 长青的内容，此属于二元分类问题。\n",
    "        -f. 分类常见算法：\n",
    "            - 决策树分类\n",
    "            - 逻辑回归分类\n",
    "            - 支持向量机分类\n",
    "            - 朴素贝叶斯分类\n",
    "        \n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    构建机器学习模型步骤；\n",
    "        1、如何搜集数据 ？\n",
    "            历史数据\n",
    "        2、如何进行数据准备 ？\n",
    "            提取特征字段和标签字段  -   特征工程    （花费时间最多的）\n",
    "        3、如何训练模型 ？\n",
    "            使用什么算法进行训练模型\n",
    "        4、如何使用模型预测 ？\n",
    "            使用训练的模型如决策树模型，进行预测\n",
    "        5、如何评估模型的准确率?\n",
    "            使用某一个标准来评估模型的准确率，二元分类中使用 AUC 作为评估标准\n",
    "        6、模型训练参数如何影响准确率？\n",
    "            训练模型时，针对算法传递不同的参数将会影响准确率和训练时间。\n",
    "            如使用决策树算法，其中参数impurity、maxDepth、maxBins的值设置\n",
    "        7、如何找出准确率最高的参数组合？\n",
    "            不同的参数，不同的组合得到的模型不一样，准确率也不痒。\n",
    "        8、如何确认是否Overfiiting（过度训练，过拟合）：\n",
    "            Overfittin（过度训练）是指机器学习所学到的模型过度贴近trainData，从而导致误差变得很大。\n",
    "            我们使用另一组数据testData再次测试，以避免overfitting的问题。\n",
    "            - 如果训练评估阶段是AUC很高，但是测试阶段AUC很低，代表可能有overfitting的问题。\n",
    "            - 如果测试与训练评估阶段的结果中AUC差异不大，就代表无overfitting的问题。\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从spark.sql中导入SparkSession类\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设置环境变量\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = 'C:\\Java\\jdk1.8.0_91'\n",
    "os.environ['HADOOP_HOME'] = 'C:\\Java\\hadoop-2.6.0-cdh5.7.6'\n",
    "os.environ['SPARK_HOME'] = 'C:\\Java\\spark-2.2.0-bin-2.6.0-cdh5.7.6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建SparkSession实例对象\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"SparkSessionExample\")\\\n",
    "    .master(\"local[3]\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取SparkContext实例对象\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### StumbleUpon 数据集\n",
    "    https://www.kaggle.com/c/stumbleupon/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 读取训练数据集\n",
    "raw_rdd = sc.textFile(\"./datas/train.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_rdd.first().split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    数据集中字段的说明：\n",
    "        -1. 字段： 0 - 2\n",
    "            表示的是url网址、urlid网址ID、boilerplate样本文字(JSON格式数据)，忽略\n",
    "        -2. 3 - 25\n",
    "            23字段属于特征字段，基本上都是数值类型，内容是有关网页相关的信息，例如：网页的分类\n",
    "            链接的数目，图像的比例等\n",
    "        -3. 字段26\n",
    "            属于标签label，具有两个值\n",
    "            - 0：表示的是长青型（evergreen）- 此网页会持续让用户感兴趣\n",
    "            - 1: 代表non-evergreen -此网页具有暂时性\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取第一条数据\n",
    "header_data = raw_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 采用过滤的方式删除第一条数据\n",
    "filter_rdd = raw_rdd.filter(lambda line: line != header_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 每天数据各个字段值，使用双引号引起来，进行替换\n",
    "datas = filter_rdd.map(lambda  line: line.replace(\"\\\"\", \"\")).map(lambda line: line.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看数据条目数\n",
    "print(datas.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datas.first()[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 提取特征features特征字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 提取类别特征字段值，使用1-of-K 编码方式转换（OneHotEncoder编码）\n",
    "\"\"\"\n",
    "1. 创建字典category_dic,一个类别对应一个数字(可以使用索引表示)\n",
    "\"\"\"\n",
    "# 构建网页分类的索引\n",
    "category_dic = datas.map(lambda fields: fields[3]).distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义第一个函数：将 ? 值进行转换为 0\n",
    "def convert_float(x):\n",
    "    return (0 if x == \"?\" else float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_float(\"34\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  导入Numpy模块\n",
    "import  numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义第二个函数：提取特征值\n",
    "def extract_features(fields, cate_dic, end_index):\n",
    "    # 分类字段\n",
    "    category_index = cate_dic[fields[3]]\n",
    "    category_features = np.zeros(len(cate_dic))\n",
    "    category_features[category_index] = 1.0 \n",
    "    \n",
    "    # 数值字段\n",
    "    numerical_features = [ convert_float(column) for column in fields[4: end_index] ]\n",
    "    \n",
    "    # 返回 “分类特征” +  “数值特征”\n",
    "    return np.concatenate((category_features, numerical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "extract_features(datas.first(), category_dic, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义第三个函数: 提取标签label\n",
    "def extract_label(fields):\n",
    "    return float(fields[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_label(datas.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征工程：构建分类算法特征数据RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelpoint_rdd = datas.map(lambda r: \n",
    "           LabeledPoint(extract_label(r), extract_features(r, category_dic, len(r) -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelpoint_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 划分数据集为：训练数据集、测试数据集、验证数据集，划分比例: 8-1-1\n",
    "(train_rdd, validation_rdd, test_rdd) = labelpoint_rdd.randomSplit([8, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各个数据集的条目数\n",
    "print \"训练数据集: \", train_rdd.cache().count()\n",
    "print \"验证数据集: \", validation_rdd.cache().count()\n",
    "print \"测试数据集: \", test_rdd.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "help(DecisionTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用决策分类算法训练数据，获取模型\n",
    "\"\"\"\n",
    "trainClassifier(cls, data, numClasses, categoricalFeaturesInfo,\n",
    "        impurity='gini', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0)\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "dtc_model = DecisionTree.trainClassifier(train_rdd, 2, {}, impurity='gini', maxDepth=5, maxBins=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    二分类模型评估：\n",
    "        http://spark.apache.org/docs/2.2.0/mllib-evaluation-metrics.html#binary-classification\n",
    "    重要的两个指标为：\n",
    "        PR 面积和ROC面积，越接近1表示模型越好\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTreeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用验证数据集 评估模型\n",
    "score = dtc_model.predict(validation_rdd.map(lambda lp: lp.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将预测与真实值进行join关联\n",
    "score_and_label = score.zip(validation_rdd.map(lambda lp: lp.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 0.0),\n",
       " (1.0, 0.0),\n",
       " (1.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_and_label.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用BinaryClassificationMetrics计算PR面积和RUC面积\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate metrics object\n",
    "metrics = BinaryClassificationMetrics(score_and_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义一个函数，用于评估二分类模型\n",
    "def model_evaluate(model, validation_datas):\n",
    "    # 导入模块\n",
    "    from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "    # 使用模型预测\n",
    "    predict_and_actual = model\\\n",
    "        .predict(validation_datas.map(lambda lp: lp.features))\\\n",
    "        .map(lambda pv: float(pv))\\\n",
    "        .zip(validation_datas.map(lambda lp: lp.label))\n",
    "    # Instantiate metrics object\n",
    "    binary_metrics = BinaryClassificationMetrics(predict_and_actual)\n",
    "    # 以AUC面积为准\n",
    "    return binary_metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用集成学习算法训练模型及预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用随机森林算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "from pyspark.mllib.tree import RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "对于随机森林算法来说：适合于特征数目非常多数据，每次构建树的时候，选取一部分特征数据进行训练构建，所以得到的树是不一样的\n",
    "def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo, numTrees,\n",
    "                        featureSubsetStrategy=\"auto\", impurity=\"gini\",\n",
    "                        maxDepth=4, maxBins=32,\n",
    "                        seed=None)\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc_model = RandomForest.trainClassifier(train_rdd, 2, {}, 10, maxDepth=5, maxBins=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估使用随机森林算法训练的模型\n",
    "model_evaluate(rfc_model, validation_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用GBT梯度提升算法训练模型并评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "from pyspark.mllib.tree import GradientBoostedTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(GradientBoostedTrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GBT 分类算法仅仅只能做二分类，多分类不行。\n",
    "     Labels should take value {0, 1}.\n",
    "trainClassifier(cls, data, categoricalFeaturesInfo, \n",
    "        loss='logLoss', numIterations=100, learningRate=0.1, maxDepth=5, maxBins=16) \n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbtc_model = GradientBoostedTrees.trainClassifier(train_rdd, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型评估\n",
    "model_evaluate(gbtc_model, validation_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "布置作业：\n",
    "    针对GBT算法来说，仿照 多层循环方式选取不同超参数的，训练模型，进行评估，获取最佳模型\n",
    "    将最佳模型进行保存，并加载模型进行预测。\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用NaiveBayes训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "from pyspark.mllib.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "针对NaiveBayes算来说，计算各个类别的概率，所以要求特征都是非负数\n",
    "    Naive Bayes requires nonnegative feature values\n",
    "\"\"\"\n",
    "# 使用训练数据集训练模型\n",
    "nb_model = NaiveBayes.train(train_rdd, lambda_=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用SVM训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_model = SVMWithSGD.train(train_rdd, iterations=100, step=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]),\n",
       " LabeledPoint(0.0, [0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.816604,2.506527415,0.637755102,0.293367347,0.091836735,0.048469388,0.592321755,0.0,0.0,0.056497175,0.0,0.223003543,0.511363636,1.0,1.0,53.0,0.0,4401.0,392.0,0.0,0.160714286,0.073684211]),\n",
       " LabeledPoint(0.0, [1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.559991,2.299492386,0.547413793,0.206896552,0.056034483,0.017241379,0.473964868,0.0,0.0,0.078431373,1.0,0.205378622,0.036423841,0.0,1.0,37.0,0.0,3610.0,232.0,11.0,0.215517241,0.080204778]),\n",
       " LabeledPoint(1.0, [0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3.417910448,0.541176471,0.270588235,0.176470588,0.117647059,0.644171779,0.0,0.0,0.075471698,0.0,0.245831092,0.166666667,1.0,1.0,68.0,0.0,536.0,85.0,6.0,0.035294118,0.159574468]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.519337,1.549450549,0.417910448,0.114427861,0.004975124,0.004975124,0.569044006,0.0,0.0,0.040764331,0.0,0.256809688,0.183206107,1.0,0.0,37.0,0.0,2382.0,201.0,1.0,0.014925373,0.092307692])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评估模型\n",
    "# 模型评估\n",
    "model_evaluate(svm_model, validation_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用模型预测\n",
    "test_rdd.map(lambda lp: svm_model.predict(lp.features)).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用LogisticRegression训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Anaconda2\\lib\\site-packages\\pyspark\\mllib\\classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "# 使用数据训练模型\n",
    "lr_model = LogisticRegressionWithSGD.train(train_rdd, iterations=100, step=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(weights=[0.294345990832,0.243158640524,-0.218702021076,-0.106112359472,-0.0103191768919,-0.00532138675276,-0.421826575898,-0.0354881009155,-0.0025968974039,-0.0884941430724,-0.355333480367,-0.0241684727468,-0.0045267380226,-0.481005707315,-0.565018766108,-2.99212523707,-0.442041993088,-0.162839306006,-0.0288427180468,-0.0129515081006,-4.82057833732,0.18852285733,0.0,-0.131246493153,-0.0458358708132,-0.303219013295,-0.541590902378,-0.746342986733,-0.696007917205,-62.9302142655,-0.0750855274487,-103.094913893,-103.414748853,-7.03683933489,-0.184208103618,-0.165350627707], intercept=0.0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rdd.map(lambda lp: lr_model.predict(lp.features)).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型评估\n",
    "model_evaluate(lr_model, validation_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "## 使用LR变形算法训练模型\n",
    "lr2_model = LogisticRegressionWithLBFGS.train(train_rdd, iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rdd.map(lambda lp: lr_model.predict(lp.features)).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluate(lr_model, validation_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
