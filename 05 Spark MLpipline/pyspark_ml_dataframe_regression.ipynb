{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "共享单车数据集：\n",
    "    http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\t1- instant: record index\n",
    "\t2- dteday : date\n",
    "\t3- season : season (1:springer, 2:summer, 3:fall, 4:winter)\n",
    "\t4- yr : year (0: 2011, 1:2012)\n",
    "\t5- mnth : month ( 1 to 12)\n",
    "\t6- hr : hour (0 to 23)\n",
    "\t7- holiday : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\n",
    "\t8- weekday : day of the week\n",
    "\t9- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "\t10+ weathersit : \n",
    "\t\t- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "\t\t- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "\t\t- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "\t\t- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "\t11- temp : Normalized temperature in Celsius. The values are divided to 41 (max)\n",
    "\t12- atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)\n",
    "\t13- hum: Normalized humidity. The values are divided to 100 (max)\n",
    "\t14- windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
    "\t15- casual: count of casual users\n",
    "\t16- registered: count of registered users\n",
    "\t17- cnt: count of total rental bikes including both casual and registered\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从spark.sql中导入SparkSession类\n",
    "from pyspark.sql import SparkSession\n",
    "# 导入系统模块\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建SparkSession实例对象\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"SparkSessionExample\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取SparkContext实例对象\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.31.109:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkSessionExample</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=SparkSessionExample>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采用spark.read.csv方式读取数据\n",
    "raw_hour_df = spark.read.csv('./hour.csv', header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- instant: integer (nullable = true)\n",
      " |-- dteday: timestamp (nullable = true)\n",
      " |-- season: integer (nullable = true)\n",
      " |-- yr: integer (nullable = true)\n",
      " |-- mnth: integer (nullable = true)\n",
      " |-- hr: integer (nullable = true)\n",
      " |-- holiday: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- workingday: integer (nullable = true)\n",
      " |-- weathersit: integer (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- casual: integer (nullable = true)\n",
      " |-- registered: integer (nullable = true)\n",
      " |-- cnt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看schema信息\n",
    "raw_hour_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "|instant|dteday             |season|yr |mnth|hr |holiday|weekday|workingday|weathersit|temp|atemp |hum |windspeed|casual|registered|cnt|\n",
      "+-------+-------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "|1      |2011-01-01 00:00:00|1     |0  |1   |0  |0      |6      |0         |1         |0.24|0.2879|0.81|0.0      |3     |13        |16 |\n",
      "|2      |2011-01-01 00:00:00|1     |0  |1   |1  |0      |6      |0         |1         |0.22|0.2727|0.8 |0.0      |8     |32        |40 |\n",
      "|3      |2011-01-01 00:00:00|1     |0  |1   |2  |0      |6      |0         |1         |0.22|0.2727|0.8 |0.0      |5     |27        |32 |\n",
      "|4      |2011-01-01 00:00:00|1     |0  |1   |3  |0      |6      |0         |1         |0.24|0.2879|0.75|0.0      |3     |10        |13 |\n",
      "|5      |2011-01-01 00:00:00|1     |0  |1   |4  |0      |6      |0         |1         |0.24|0.2879|0.75|0.0      |0     |1         |1  |\n",
      "|6      |2011-01-01 00:00:00|1     |0  |1   |5  |0      |6      |0         |2         |0.24|0.2576|0.75|0.0896   |0     |1         |1  |\n",
      "|7      |2011-01-01 00:00:00|1     |0  |1   |6  |0      |6      |0         |1         |0.22|0.2727|0.8 |0.0      |2     |0         |2  |\n",
      "|8      |2011-01-01 00:00:00|1     |0  |1   |7  |0      |6      |0         |1         |0.2 |0.2576|0.86|0.0      |1     |2         |3  |\n",
      "|9      |2011-01-01 00:00:00|1     |0  |1   |8  |0      |6      |0         |1         |0.24|0.2879|0.75|0.0      |1     |7         |8  |\n",
      "|10     |2011-01-01 00:00:00|1     |0  |1   |9  |0      |6      |0         |1         |0.32|0.3485|0.76|0.0      |8     |6         |14 |\n",
      "+-------+-------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 显示10条数据\n",
    "raw_hour_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17379\n"
     ]
    }
   ],
   "source": [
    "print(raw_hour_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instant', 'dteday', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt']\n"
     ]
    }
   ],
   "source": [
    "# 查看列的名称\n",
    "print(raw_hour_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征提取\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选取要是字段特征\n",
    "\"\"\"\n",
    "综合分析：instant(序号)、dteday（日期）、yr（年份）、casual（非注册用户租车树）和registered（注册用户租车数）\n",
    "\"\"\"\n",
    "hour_df = raw_hour_df.drop('instant').drop('dteday').drop('yr').drop('casual').drop('registered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: integer (nullable = true)\n",
      " |-- mnth: integer (nullable = true)\n",
      " |-- hr: integer (nullable = true)\n",
      " |-- holiday: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- workingday: integer (nullable = true)\n",
      " |-- weathersit: integer (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- cnt: integer (nullable = true)\n",
      "\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+---+\n",
      "|season|mnth|hr |holiday|weekday|workingday|weathersit|temp|atemp |hum |windspeed|cnt|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+---+\n",
      "|1     |1   |0  |0      |6      |0         |1         |0.24|0.2879|0.81|0.0      |16 |\n",
      "|1     |1   |1  |0      |6      |0         |1         |0.22|0.2727|0.8 |0.0      |40 |\n",
      "|1     |1   |2  |0      |6      |0         |1         |0.22|0.2727|0.8 |0.0      |32 |\n",
      "|1     |1   |3  |0      |6      |0         |1         |0.24|0.2879|0.75|0.0      |13 |\n",
      "|1     |1   |4  |0      |6      |0         |1         |0.24|0.2879|0.75|0.0      |1  |\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hour_df.printSchema()\n",
    "\n",
    "hour_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "将上述所有字段的数据类型数值类型，double类型, 针对SparkMLlib中分类和回归算法数据类型Double数值类型\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col\n",
    "## 转换 alias 回去\n",
    "bike_hour_df = hour_df.select([ col(column).cast('double').alias(column) for column in hour_df.columns ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: double (nullable = true)\n",
      " |-- mnth: double (nullable = true)\n",
      " |-- hr: double (nullable = true)\n",
      " |-- holiday: double (nullable = true)\n",
      " |-- weekday: double (nullable = true)\n",
      " |-- workingday: double (nullable = true)\n",
      " |-- weathersit: double (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- cnt: double (nullable = true)\n",
      "\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+\n",
      "|season|mnth|hr |holiday|weekday|workingday|weathersit|temp|atemp |hum |windspeed|cnt |\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+\n",
      "|1.0   |1.0 |0.0|0.0    |6.0    |0.0       |1.0       |0.24|0.2879|0.81|0.0      |16.0|\n",
      "|1.0   |1.0 |1.0|0.0    |6.0    |0.0       |1.0       |0.22|0.2727|0.8 |0.0      |40.0|\n",
      "|1.0   |1.0 |2.0|0.0    |6.0    |0.0       |1.0       |0.22|0.2727|0.8 |0.0      |32.0|\n",
      "|1.0   |1.0 |3.0|0.0    |6.0    |0.0       |1.0       |0.24|0.2879|0.75|0.0      |13.0|\n",
      "|1.0   |1.0 |4.0|0.0    |6.0    |0.0       |1.0       |0.24|0.2879|0.75|0.0      |1.0 |\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_hour_df.printSchema()\n",
    "bike_hour_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组合特征字段为Vector向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n"
     ]
    }
   ],
   "source": [
    "features_cols = bike_hour_df.columns[:-1]\n",
    "\n",
    "print(features_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组合特征到向量\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features_cols,\n",
    "    outputCol=\"raw_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 针对DataFrame中数据进行转换\n",
    "bike_hour_df2 = assembler.transform(bike_hour_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: double (nullable = true)\n",
      " |-- mnth: double (nullable = true)\n",
      " |-- hr: double (nullable = true)\n",
      " |-- holiday: double (nullable = true)\n",
      " |-- weekday: double (nullable = true)\n",
      " |-- workingday: double (nullable = true)\n",
      " |-- weathersit: double (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- cnt: double (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      "\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+--------------------------------------------------+\n",
      "|season|mnth|hr |holiday|weekday|workingday|weathersit|temp|atemp |hum |windspeed|cnt |raw_features                                      |\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+--------------------------------------------------+\n",
      "|1.0   |1.0 |0.0|0.0    |6.0    |0.0       |1.0       |0.24|0.2879|0.81|0.0      |16.0|[1.0,1.0,0.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.81,0.0]|\n",
      "|1.0   |1.0 |1.0|0.0    |6.0    |0.0       |1.0       |0.22|0.2727|0.8 |0.0      |40.0|[1.0,1.0,1.0,0.0,6.0,0.0,1.0,0.22,0.2727,0.8,0.0] |\n",
      "|1.0   |1.0 |2.0|0.0    |6.0    |0.0       |1.0       |0.22|0.2727|0.8 |0.0      |32.0|[1.0,1.0,2.0,0.0,6.0,0.0,1.0,0.22,0.2727,0.8,0.0] |\n",
      "|1.0   |1.0 |3.0|0.0    |6.0    |0.0       |1.0       |0.24|0.2879|0.75|0.0      |13.0|[1.0,1.0,3.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.75,0.0]|\n",
      "|1.0   |1.0 |4.0|0.0    |6.0    |0.0       |1.0       |0.24|0.2879|0.75|0.0      |1.0 |[1.0,1.0,4.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.75,0.0]|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_hour_df2.printSchema()\n",
    "bike_hour_df2.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用决策树回归算法模型（模型学习器）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "决策树回归算法默认超参数的值：\n",
    "featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                 maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,\n",
    "     maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity=\"variance\",\n",
    "                 seed=None, varianceCol=None\n",
    "\"\"\"\n",
    "dtr = DecisionTreeRegressor(maxDepth=10, maxBins=32, \n",
    "                            featuresCol=\"raw_features\", labelCol=\"cnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用数据集训练模型\n",
    "dtr_model = dtr.fit(bike_hour_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_bea18ee11e9a) of depth 10 with 1785 nodes"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(11, {0: 0.024, 1: 0.0054, 2: 0.6885, 3: 0.0029, 4: 0.0072, 5: 0.0742, 6: 0.021, 7: 0.1357, 8: 0.0107, 9: 0.0256, 10: 0.0048})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr_model.numFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1785"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr_model.numNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_hour_df = dtr_model.transform(bike_hour_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: double (nullable = true)\n",
      " |-- mnth: double (nullable = true)\n",
      " |-- hr: double (nullable = true)\n",
      " |-- holiday: double (nullable = true)\n",
      " |-- weekday: double (nullable = true)\n",
      " |-- workingday: double (nullable = true)\n",
      " |-- weathersit: double (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- cnt: double (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_hour_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+----+------------------+\n",
      "|raw_features                                      |cnt |prediction        |\n",
      "+--------------------------------------------------+----+------------------+\n",
      "|[1.0,1.0,0.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.81,0.0]|16.0|37.22222222222222 |\n",
      "|[1.0,1.0,1.0,0.0,6.0,0.0,1.0,0.22,0.2727,0.8,0.0] |40.0|37.22222222222222 |\n",
      "|[1.0,1.0,2.0,0.0,6.0,0.0,1.0,0.22,0.2727,0.8,0.0] |32.0|23.88095238095238 |\n",
      "|[1.0,1.0,3.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.75,0.0]|13.0|12.030303030303031|\n",
      "|[1.0,1.0,4.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.75,0.0]|1.0 |4.055555555555555 |\n",
      "+--------------------------------------------------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_hour_df.select('raw_features', 'cnt', 'prediction').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# 构建评估器实例对象\n",
    "evaluator = RegressionEvaluator(labelCol='cnt', predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.41731834779505"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predict_hour_df, {evaluator.metricName: 'rmse'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对类别特征处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: double (nullable = true)\n",
      " |-- mnth: double (nullable = true)\n",
      " |-- hr: double (nullable = true)\n",
      " |-- holiday: double (nullable = true)\n",
      " |-- weekday: double (nullable = true)\n",
      " |-- workingday: double (nullable = true)\n",
      " |-- weathersit: double (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- cnt: double (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      "\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+--------------------------------------------------+\n",
      "|season|mnth|hr |holiday|weekday|workingday|weathersit|temp|atemp |hum |windspeed|cnt |raw_features                                      |\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+--------------------------------------------------+\n",
      "|1.0   |1.0 |0.0|0.0    |6.0    |0.0       |1.0       |0.24|0.2879|0.81|0.0      |16.0|[1.0,1.0,0.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.81,0.0]|\n",
      "|1.0   |1.0 |1.0|0.0    |6.0    |0.0       |1.0       |0.22|0.2727|0.8 |0.0      |40.0|[1.0,1.0,1.0,0.0,6.0,0.0,1.0,0.22,0.2727,0.8,0.0] |\n",
      "|1.0   |1.0 |2.0|0.0    |6.0    |0.0       |1.0       |0.22|0.2727|0.8 |0.0      |32.0|[1.0,1.0,2.0,0.0,6.0,0.0,1.0,0.22,0.2727,0.8,0.0] |\n",
      "|1.0   |1.0 |3.0|0.0    |6.0    |0.0       |1.0       |0.24|0.2879|0.75|0.0      |13.0|[1.0,1.0,3.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.75,0.0]|\n",
      "|1.0   |1.0 |4.0|0.0    |6.0    |0.0       |1.0       |0.24|0.2879|0.75|0.0      |1.0 |[1.0,1.0,4.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.75,0.0]|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_hour_df2.printSchema()\n",
    "bike_hour_df2.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VectorIndexer:\n",
    "    对类别特征数据进行处理，使用类别特征的类别数目的下标作为次类别特征的值\n",
    "\"\"\"\n",
    "from pyspark.ml.feature import VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = VectorIndexer(inputCol=\"raw_features\", outputCol=\"features\", maxCategories=24)\n",
    "# 使用数据得到的 模型学习器\n",
    "indexer_model = indexer.fit(bike_hour_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行数据转换\n",
    "bike_hour_df3 = indexer_model.transform(bike_hour_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: double (nullable = true)\n",
      " |-- mnth: double (nullable = true)\n",
      " |-- hr: double (nullable = true)\n",
      " |-- holiday: double (nullable = true)\n",
      " |-- weekday: double (nullable = true)\n",
      " |-- workingday: double (nullable = true)\n",
      " |-- weathersit: double (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- cnt: double (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_hour_df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------------------------------------+--------------------------------------------------+\n",
      "|cnt |raw_features                                      |features                                          |\n",
      "+----+--------------------------------------------------+--------------------------------------------------+\n",
      "|16.0|[1.0,1.0,0.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.81,0.0]|[0.0,0.0,0.0,0.0,6.0,0.0,0.0,0.24,0.2879,0.81,0.0]|\n",
      "|40.0|[1.0,1.0,1.0,0.0,6.0,0.0,1.0,0.22,0.2727,0.8,0.0] |[0.0,0.0,1.0,0.0,6.0,0.0,0.0,0.22,0.2727,0.8,0.0] |\n",
      "|32.0|[1.0,1.0,2.0,0.0,6.0,0.0,1.0,0.22,0.2727,0.8,0.0] |[0.0,0.0,2.0,0.0,6.0,0.0,0.0,0.22,0.2727,0.8,0.0] |\n",
      "|13.0|[1.0,1.0,3.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.75,0.0]|[0.0,0.0,3.0,0.0,6.0,0.0,0.0,0.24,0.2879,0.75,0.0]|\n",
      "|1.0 |[1.0,1.0,4.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.75,0.0]|[0.0,0.0,4.0,0.0,6.0,0.0,0.0,0.24,0.2879,0.75,0.0]|\n",
      "+----+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_hour_df3.select('cnt', 'raw_features', 'features').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.10943240563866"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练\n",
    "dtr2 = DecisionTreeRegressor(maxDepth=10, maxBins=32, \n",
    "                            featuresCol=\"features\", labelCol=\"cnt\")\n",
    "dtr_model2 = dtr2.fit(bike_hour_df3)\n",
    "# 预测\n",
    "predict_hour_df2 = dtr_model2.transform(bike_hour_df3)\n",
    "\n",
    "# 评估\n",
    "evaluator.evaluate(predict_hour_df2, {evaluator.metricName: 'rmse'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|cnt |prediction        |\n",
      "+----+------------------+\n",
      "|16.0|47.15384615384615 |\n",
      "|40.0|47.15384615384615 |\n",
      "|32.0|47.5              |\n",
      "|13.0|10.608695652173912|\n",
      "|1.0 |5.25              |\n",
      "+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_hour_df2.select('cnt', 'prediction').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何找出最佳模型（调整算法超参数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "在Spark ML机器学习库中，提供如何找到最佳模型：\n",
    "    http://spark.apache.org/docs/2.2.0/ml-tuning.html#ml-tuning-model-selection-and-hyperparameter-tuning\n",
    "    -a. 交叉验证\n",
    "        Cross-Validation\n",
    "    -b. 训练验证分隔\n",
    "        Train-Validation Split\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用Train-Validation 找出最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"cnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建算法超参数\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(dt.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(dt.maxBins, [32, 64, 128])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建评估器\n",
    "regeression_evaluator = RegressionEvaluator(labelCol='cnt', predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练验证分隔实例对象\n",
    "dt_tvs = TrainValidationSplit(estimator=dt,estimatorParamMaps=paramGrid,\n",
    "                           evaluator=regeression_evaluator, trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用数据训练和验证模型，使用不同超参数组合\n",
    "dt_best_model = dt_tvs.fit(bike_hour_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_c81bb2e8010d) of depth 10 with 1831 nodes"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_best_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.68407783551991"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评估\n",
    "regeression_evaluator.evaluate(\n",
    "    dt_best_model.bestModel.transform(bike_hour_df3), {evaluator.metricName: 'rmse'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best_model.bestModel.save('./best_dt_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用Cross-Validation找出最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "paramGrid2 = ParamGridBuilder()\\\n",
    "    .addGrid(dt.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(dt.maxBins, [32, 64, 128])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2 = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"cnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建评估器\n",
    "regeression_evaluator2 = RegressionEvaluator(labelCol='cnt', predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建交叉验证的实例对象\n",
    "crossval = CrossValidator(estimator=dt2,\n",
    "                          estimatorParamMaps=paramGrid2,\n",
    "                          evaluator=regeression_evaluator2,\n",
    "                          numFolds=5)  # use 3+ folds in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = crossval.fit(bike_hour_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_3cf553441924) of depth 5 with 63 nodes"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.61353642096543"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评估\n",
    "regeression_evaluator.evaluate(\n",
    "    cv_model.bestModel.transform(bike_hour_df3), {evaluator.metricName: 'rmse'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Pipeline组合机器学习开发流程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "回顾一下，上述机器学习过程中流程步骤：\n",
    "    -0. 读取数据, 适当数据处理\n",
    "        bike_hour_df\n",
    "    -1. 组合特征值到Vector中\n",
    "        VectorAssembler\n",
    "    -2. 处理类别特征数据\n",
    "         VectorIndexer\n",
    "    -3. 使用决策树训练模型\n",
    "        DecisionTreeRegressor\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.regression import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. 构建 特征合并VectorAssembler\n",
    "vector_assembler = VectorAssembler(inputCols=features_cols, outputCol='raw_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. 创建类别特征VectorIndexer\n",
    "vector_indexer = VectorIndexer(inputCol='raw_features', \n",
    "                               outputCol='features', maxCategories=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. 构建决策树回归实例对象\n",
    "dt_regression = DecisionTreeRegressor(labelCol='cnt', featuresCol='features', \n",
    "                                      maxDepth=5, maxBins=64, varianceCol='variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "# 构建Pipeline\n",
    "dtr_pipeline = Pipeline(stages=[vector_assembler, vector_indexer, dt_regression])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorAssembler_338cd010e37a,\n",
       " VectorIndexer_1dc90421dde4,\n",
       " DecisionTreeRegressor_e2b146e4472f]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr_pipeline.getStages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据应用到Pipeline中，训练模型\n",
    "dtr_pipeline_model = dtr_pipeline.fit(bike_hour_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_e2b146e4472f) of depth 5 with 63 nodes\n",
      "  If (feature 2 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,22.0,23.0})\n",
      "   If (feature 2 in {0.0,1.0,2.0,3.0,4.0,5.0})\n",
      "    If (feature 2 in {2.0,3.0,4.0,5.0})\n",
      "     If (feature 4 in {1.0,2.0,3.0,4.0,5.0})\n",
      "      If (feature 2 in {2.0,3.0,4.0})\n",
      "       Predict: 6.705921938088829\n",
      "      Else (feature 2 not in {2.0,3.0,4.0})\n",
      "       Predict: 24.33139534883721\n",
      "     Else (feature 4 not in {1.0,2.0,3.0,4.0,5.0})\n",
      "      If (feature 2 in {3.0,4.0,5.0})\n",
      "       Predict: 14.760129659643436\n",
      "      Else (feature 2 not in {3.0,4.0,5.0})\n",
      "       Predict: 55.98067632850242\n",
      "    Else (feature 2 not in {2.0,3.0,4.0,5.0})\n",
      "     If (feature 4 in {1.0,2.0,3.0,4.0,5.0})\n",
      "      If (feature 2 in {1.0})\n",
      "       Predict: 17.464077669902913\n",
      "      Else (feature 2 not in {1.0})\n",
      "       Predict: 37.62669245647969\n",
      "     Else (feature 4 not in {1.0,2.0,3.0,4.0,5.0})\n",
      "      If (feature 1 in {0.0,1.0,2.0,3.0,10.0,11.0})\n",
      "       Predict: 57.404761904761905\n",
      "      Else (feature 1 not in {0.0,1.0,2.0,3.0,10.0,11.0})\n",
      "       Predict: 109.57692307692308\n",
      "   Else (feature 2 not in {0.0,1.0,2.0,3.0,4.0,5.0})\n",
      "    If (feature 7 <= 0.43)\n",
      "     If (feature 0 in {0.0})\n",
      "      If (feature 7 <= 0.27)\n",
      "       Predict: 36.534013605442176\n",
      "      Else (feature 7 > 0.27)\n",
      "       Predict: 59.76842105263158\n",
      "     Else (feature 0 not in {0.0})\n",
      "      If (feature 5 in {0.0})\n",
      "       Predict: 54.104294478527606\n",
      "      Else (feature 5 not in {0.0})\n",
      "       Predict: 93.26851851851852\n",
      "    Else (feature 7 > 0.43)\n",
      "     If (feature 2 in {6.0,23.0})\n",
      "      If (feature 4 in {0.0})\n",
      "       Predict: 53.42201834862385\n",
      "      Else (feature 4 not in {0.0})\n",
      "       Predict: 112.88148148148149\n",
      "     Else (feature 2 not in {6.0,23.0})\n",
      "      If (feature 8 <= 0.58335)\n",
      "       Predict: 143.9090909090909\n",
      "      Else (feature 8 > 0.58335)\n",
      "       Predict: 191.036866359447\n",
      "  Else (feature 2 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,22.0,23.0})\n",
      "   If (feature 7 <= 0.47)\n",
      "    If (feature 2 in {7.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,19.0,20.0,21.0})\n",
      "     If (feature 0 in {0.0})\n",
      "      If (feature 7 <= 0.25)\n",
      "       Predict: 87.59944367176634\n",
      "      Else (feature 7 > 0.25)\n",
      "       Predict: 147.0990990990991\n",
      "     Else (feature 0 not in {0.0})\n",
      "      If (feature 6 in {2.0})\n",
      "       Predict: 101.08018867924528\n",
      "      Else (feature 6 not in {2.0})\n",
      "       Predict: 218.56751592356687\n",
      "    Else (feature 2 not in {7.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,19.0,20.0,21.0})\n",
      "     If (feature 5 in {0.0})\n",
      "      If (feature 7 <= 0.33)\n",
      "       Predict: 85.61818181818182\n",
      "      Else (feature 7 > 0.33)\n",
      "       Predict: 181.1552795031056\n",
      "     Else (feature 5 not in {0.0})\n",
      "      If (feature 0 in {0.0})\n",
      "       Predict: 282.5632911392405\n",
      "      Else (feature 0 not in {0.0})\n",
      "       Predict: 455.3733766233766\n",
      "   Else (feature 7 > 0.47)\n",
      "    If (feature 2 in {7.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,20.0,21.0})\n",
      "     If (feature 5 in {1.0})\n",
      "      If (feature 2 in {9.0,10.0,11.0,12.0,13.0,14.0,15.0,21.0})\n",
      "       Predict: 219.0162567736557\n",
      "      Else (feature 2 not in {9.0,10.0,11.0,12.0,13.0,14.0,15.0,21.0})\n",
      "       Predict: 337.21445497630333\n",
      "     Else (feature 5 not in {1.0})\n",
      "      If (feature 2 in {7.0,9.0,20.0,21.0})\n",
      "       Predict: 191.22654462242562\n",
      "      Else (feature 2 not in {7.0,9.0,20.0,21.0})\n",
      "       Predict: 432.62046204620464\n",
      "    Else (feature 2 not in {7.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,20.0,21.0})\n",
      "     If (feature 5 in {0.0})\n",
      "      If (feature 2 in {8.0})\n",
      "       Predict: 140.92929292929293\n",
      "      Else (feature 2 not in {8.0})\n",
      "       Predict: 374.0127226463104\n",
      "     Else (feature 5 not in {0.0})\n",
      "      If (feature 2 in {19.0})\n",
      "       Predict: 431.3\n",
      "      Else (feature 2 not in {19.0})\n",
      "       Predict: 591.8285385500575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dtr_pipeline_model.stages[2].toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用模型预测\n",
    "predict_hour_df_pl = dtr_pipeline_model.transform(bike_hour_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------------------+\n",
      "|cnt |prediction        |variance          |\n",
      "+----+------------------+------------------+\n",
      "|16.0|57.404761904761905|1062.1552154195012|\n",
      "|40.0|57.404761904761905|1062.1552154195012|\n",
      "|32.0|55.98067632850242 |717.7290951947537 |\n",
      "|13.0|14.760129659643436|171.6863844240312 |\n",
      "|1.0 |14.760129659643436|171.6863844240312 |\n",
      "|1.0 |14.760129659643436|171.6863844240312 |\n",
      "|2.0 |36.534013605442176|680.330475727706  |\n",
      "|3.0 |87.59944367176634 |3196.9605560187324|\n",
      "|8.0 |85.61818181818182 |2800.91482093664  |\n",
      "|14.0|147.0990990990991 |7560.885347263725 |\n",
      "+----+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_hour_df_pl.select('cnt', 'prediction', 'variance').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_pipeline_model.save('./pl-dtr-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
